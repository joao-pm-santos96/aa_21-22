{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab  - LSTM for Air Polution Forecasting\n",
    "\n",
    "\n",
    "\n",
    "**Objectives**: Apply Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) for \n",
    "Multivariate Time Series Forecasting. \n",
    "\n",
    "**The forecasting problem**: Air Quality dataset reports on the weather and the level of pollution each hour over 5 years at the US embassy in Beijing, China. Given the weather conditions and pollution for prior hours, forecast the pollution at the next hour.\n",
    "\n",
    "Data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete list of data is as follows:\n",
    "\n",
    "**Total data**: 43800 samples (5 years x 365 days x 24 hours), 8 features.\n",
    "\n",
    "**year-month-day-hour**: date of data in this row\n",
    "\n",
    "**pm2.5**: PM2.5 concentration  (**pollution indicator => the predicted variable**) \n",
    "\n",
    "**DEWP**: Dew Point (DewP indicates the amount moisture in the air. The higher the dewP, the higher the moisture content of the air at a given temperature.\n",
    "\n",
    "**TEM**P: Temperature\n",
    "\n",
    "**PRES**: Pressure\n",
    "\n",
    "**cbwd**: Combined wind direction (categorical feature, 4 values, e.g. Sought-East (SE))\n",
    "\n",
    "**Iws**: Cumulated wind speed\n",
    "\n",
    "**Is**: Cumulated hours of snow\n",
    "\n",
    "**Ir**: Cumulated hours of rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=RuntimeWarning)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Use function *read_csv* to load data from file *pollution.csv*. \n",
    "This function will create dataframe dataset. \n",
    "\n",
    "Dataframe consists not only of values but also \n",
    "the names of the columns, the indices of the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_12868/1156196339.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\hp\\AppData\\Local\\Temp/ipykernel_12868/1156196339.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    dataset =  ?\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset =  ?\n",
    "\n",
    "#Print the dimension of dataset \n",
    "?\n",
    "\n",
    "#Print the first few lines of the structure dataset \n",
    "?\n",
    "\n",
    "#Print only the names of the columns\n",
    "?\n",
    "\n",
    "n_features =  ?  #number of features \n",
    "\n",
    "# Extract only the values of dataframe and ignore the column with the date. \n",
    "values = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "Plot the data and get a figure similar to Fig.1 \n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : ** Pollution & Weather Time series dataset** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##use for example pyplot\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data Preparation\n",
    "Prepare the dataset for the LSTM, that is, frame the dataset as a supervised learning problem to predict the pollution (pm2.5) at the current or future hours given the past pollution and weather conditions over a number of prior time steps (hours).\n",
    "\n",
    "\n",
    "**Original data frame (each row has data of only that hour)** :\n",
    "\n",
    "row 1=> var1(t), var2(t),...var8(t)\n",
    "\n",
    "row 2=> var1(t-1), var2(t-1),...var8(t)\n",
    "\n",
    "row 3=> .....\n",
    "\n",
    "**After series_to_supervised (each row has data of n previous hours and the current hour)**:\n",
    "\n",
    "\n",
    "row 1=> var1(t-n), var2(t-n),...var8(t-n), var1(t-n+1), var2(t-n+1),...var8(t-n+1), .....var1(t), var3(t),...var8(t)\n",
    "\n",
    "row 2=> var1(t-n-1), var2(t-n-1),...var8(t-n-1), var1(t-1), var3(t-1),...var8(t-1)\n",
    "\n",
    "row 3 => ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): \n",
    "    \"\"\"\n",
    "        data: data matrix\n",
    "        n_in: number of input lag timesteps (hours)\n",
    "        n_out: number of output prediction timesteps (hours)\n",
    " \n",
    "    \"\"\"\n",
    "        \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)  #transforms data (pure matrix of values) into data structure adding index of rows and columns\n",
    "    cols, names = list(), list()  #inicialize empty lists \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical feature cbwd: Combined wind direction (4 categorical values) into integers (0,1,2,3)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4]) # transform the 5th column into integers 0,1,2,3\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled =  ?  #apply scaler to the values \n",
    "\n",
    "\n",
    "# specify the number of lag hours. You can change this hyper-parameter\n",
    "lag_hours = 3\n",
    "\n",
    "# call series_to_supervised to frame data as supervised learning\n",
    "reframed = ?\n",
    "\n",
    "#Print the dimension of  the dataframe reframed: ANSWER=(43797, 32) \n",
    "?\n",
    "\n",
    "#Print the first few lines of reframed\n",
    "?\n",
    "\n",
    "# we want to predict only the feature pm2.5 =var1(t)\n",
    "# Take out the columns of var2(t), var3(t),...var8(t).\n",
    "reframed.drop(?)\n",
    "\n",
    "#Print the new dimension of  reframed: ANSWER=(43797, 25) \n",
    "?\n",
    "\n",
    "#Print the first few lines of the new reframed\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "First, data is split into train and test subsets. To speed up the model fiting, the training data is only the \n",
    "1st year, the test data are the remaining 4 years. You may consider exploring the oposite. \n",
    "\n",
    "Then the train and test sets are split into input and output variables. \n",
    "\n",
    "Finally, the inputs (X) are reshaped into 3D format expected by LSTMs, namely [samples, timesteps, features].\n",
    "\n",
    "**Total data**: 43800 samples (5 years x 365 days x 24 hours), 8 features.\n",
    "\n",
    "**Train data**: 3D tensor(samples, timesteps, features).\n",
    "8760 samples:  Data from 1 year (365 days), 24 times per day (1h period).\n",
    "\n",
    "**Test data**: 3D tensor (samples, timesteps, features). The rest of 35037 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "# reframed is a dataframe that has not only values but also heads and row indixes. \n",
    "#Extract only the values \n",
    "\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24  #8760 (1 year, 24 times per day, i.e. period 1h)\n",
    "\n",
    "#Extract training subset from values \n",
    "train = ?\n",
    "\n",
    "#Extract test subset from values \n",
    "test = ?\n",
    "\n",
    "train_X  = ?   # train_X.shape = (8760, 24)\n",
    "train_y = ?  # train_y.shape = (8760,)\n",
    "\n",
    "test_X  =  ?      # test_X.shape = (35037, 24)\n",
    "test_y =  ? # test_y.shape = (35037,)\n",
    "\n",
    "# reshape input to be 3D [samples, lag_hours, features]\n",
    "train_X = ? # train_X.shape=(8760,3,8)\n",
    "test_X =  ?  # test_X.shape=(35037,3,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Fit Model\n",
    "\n",
    "In this section, we define and fit an LSTM on the multivariate input data.\n",
    "We define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution.\n",
    "We use the Mean Absolute Error (MAE) loss function and the Adam version of stochastic gradient descent.\n",
    "The model will be fit for 50 training epochs with a batch size of 72. \n",
    "At the end of the run both the training and test loss are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model is appropriate to design LSTM RNN\n",
    "model = Sequential()\n",
    "# train_X : 3D tensor (samples, lag_hours, feature)\n",
    "# neurons =50\n",
    "# single input_shape = (lag_hours, feature)\n",
    "model.add(LSTM(50, input_shape=(?,?)))\n",
    "model.add(Dense(1))  # 1 neuron in the output layer\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# fit network\n",
    "#verbose=0 will show nothing (silent)\n",
    "#verbose=1 will show an animated progress bar\n",
    "#verbose=2 will show the training progress for each epoch (shows loss and val_loss)\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "#  print history\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.title('Loss function')\n",
    "pyplot.legend(['train', 'test'])\n",
    "pyplot.xlabel('iterations')\n",
    "pyplot.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "After the model is fit, apply it to predict test data.\n",
    "Compare the predictions (yhat) and the real data (test_y) as shown in Fig. 2. \n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : ** Test data prediction (the first 2000 samples)** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the model to predict the test data\n",
    "yhat = ?\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) on test data with function mean_squared_error \n",
    "#(ANSWER: Test mse = 0.001)\n",
    "mse = ?\n",
    "\n",
    "\n",
    "#Plot test_y and yhat on the same plot. \n",
    "#use pyplot from matplotlib\n",
    "#Since test data has huge amount of samples (35037), the plot visibility is bad. \n",
    "#To improve it choose to plot only the fist 2000 samples.\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the same as the previous cell but now with train data\n",
    "?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
